---
layout: post
title:  "loss funtion"
excerpt: "tensorflow loss 연습"
toc: true
toc_sticky: true
author_profile: true

categories:
  - DL
tags:
  - Python
  - tensorflow
  - loss
  - partial
--- 

```python
import tensorflow as tf
```


```python
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
```


```python
tf.nn.sigmoid_cross_entropy_with_logits

# logit : activation 들어가기 전에 결과값
```


```python
tf.keras.losses.binary_crossentropy # 함수 : 함수형 패러다임
tf.keras.losses.BinaryCrossentropy # 클래스 : 객체지향(함수형 패러다임 숨어 있음)
```


```python
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(10) # softmax 는 numerical stability 보장 안됨: 
                            # softmax 안쓴다는 것은 logit 개념으로 softmax 들어가기 전에 값을 가지고 와서 사용 한다는 의미, 
                            # 논리니어티가 아니라 readout
                            # tf.nn.sigmoid_cross_entropy_with_logits 사용하면 numerical stability 보장 되기 때문에 
])
```


```python
input_ = tf.keras.Input((28,28))
x = tf.keras.layers.Flatten()(input_)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dense(10)(x)
```


```python
model = tf.keras.Model(input_, x)
```


```python
model.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_1 (InputLayer)        [(None, 28, 28)]          0         
                                                                     
     flatten (Flatten)           (None, 784)               0         
                                                                     
     dense (Dense)               (None, 128)               100480    
                                                                     
     dense_1 (Dense)             (None, 10)                1290      
                                                                     
    =================================================================
    Total params: 101,770
    Trainable params: 101,770
    Non-trainable params: 0
    _________________________________________________________________



```python
# 1. argument 변경시
loss1 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
```


```python
# 2. () 없이 > agument 변경할려면 
# 2-1. agument 변경할려면 partial 테크닉
# 2-2. nested function
loss = tf.keras.losses.sparse_categorical_crossentropy
```


```python
from functools import partial
loss2 = partial(tf.keras.losses.sparse_categorical_crossentropy, from_logits=True)
```


```python
def loss3(from_logits=True):
    def y(y_true, y_pred):
        return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=from_logits)
    return y
```


```python
# 3. string
model6.compile(loss='sparse_categorical_crossentropy')
```


```python
loss4 = tf.nn.softmax_cross_entropy_with_logits # one-hot encoding 가정
# = tf.keras.losses.CategoricalCrossentropy
```


```python
model.compile(loss=loss1)
```


```python
model.fit(X_train, y_train)
```

    1875/1875 [==============================] - 14s 6ms/step - loss: 496111.7812





    <keras.callbacks.History at 0x1cd09de8850>




```python
model.compile(loss=loss2)
```


```python
model.fit(X_train, y_train)
```

    1875/1875 [==============================] - 11s 6ms/step - loss: 2.7764





    <keras.callbacks.History at 0x1cd17ed9d60>




```python
model.compile(loss=loss3())
```


```python
model.fit(X_train, y_train)
```

    1875/1875 [==============================] - 11s 6ms/step - loss: 2.6502





    <keras.callbacks.History at 0x1cc97e62730>




```python
model.compile(loss=loss4)
```


```python
model.fit(X_train, y_train) # loss4
```

    1875/1875 [==============================] - 12s 6ms/step - loss: 733138.9375





    <keras.callbacks.History at 0x1cc97f27910>




```python
loss5 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) # softmax 안쓰고 false 값을 쓰면 loss 학습이 잘 안됨 아래 loss1 비교
model.compile(loss=loss5)
model.fit(X_train, y_train, epochs=5)
```

    Epoch 1/5
    1875/1875 [==============================] - 12s 6ms/step - loss: 3.8974
    Epoch 2/5
    1875/1875 [==============================] - 11s 6ms/step - loss: 2.3095
    Epoch 3/5
    1875/1875 [==============================] - 11s 6ms/step - loss: 2.3037
    Epoch 4/5
    1875/1875 [==============================] - 11s 6ms/step - loss: 2.3026
    Epoch 5/5
    1875/1875 [==============================] - 12s 6ms/step - loss: 2.3026





    <keras.callbacks.History at 0x1cd11c5cdc0>




```python
loss1 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(loss=loss1)
model.fit(X_train, y_train, epochs=5)
```

    Epoch 1/5
    1875/1875 [==============================] - 12s 6ms/step - loss: 3.0797
    Epoch 2/5
    1875/1875 [==============================] - 11s 6ms/step - loss: 0.5858
    Epoch 3/5
    1875/1875 [==============================] - 11s 6ms/step - loss: 0.4708
    Epoch 4/5
    1875/1875 [==============================] - 10s 5ms/step - loss: 0.4371
    Epoch 5/5
    1875/1875 [==============================] - 10s 5ms/step - loss: 0.3952





    <keras.callbacks.History at 0x1cd225b9c70>




```python
X_train = X_train / 255.0 # min max normalization 했을 때 loss 값 시작이 0.25 위에 안했을 때는 3.07
```


```python
loss1 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(loss=loss1)
model.fit(X_train, y_train, epochs=5)
```

    Epoch 1/5
    1875/1875 [==============================] - 12s 6ms/step - loss: 0.2545
    Epoch 2/5
    1875/1875 [==============================] - 11s 6ms/step - loss: 0.1194
    Epoch 3/5
    1875/1875 [==============================] - 10s 5ms/step - loss: 0.0877
    Epoch 4/5
    1875/1875 [==============================] - 9s 5ms/step - loss: 0.0717
    Epoch 5/5
    1875/1875 [==============================] - 9s 5ms/step - loss: 0.0611





    <keras.callbacks.History at 0x1cd263b7cd0>


