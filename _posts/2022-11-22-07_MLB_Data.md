---
title:  "MLB ë°ì´í„°ë¥¼ í™œìš©í•œ íšŒê·€ëª¨ë¸ ìƒì„± ë° ê²°ê³¼ë¶„ì„"
excerpt: "Byte Degree : êµ­ë¯¼ëŒ€ x íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ ë¨¸ì‹ ëŸ¬ë‹ ì‹¤ë¬´ ì¸ì¦ ê³¼ì •"

categories:
  - ML
tags:
  - ByteDegree
  - Python
  - íšŒê·€ë¶„ì„
--- 

---
# í”„ë¡œì íŠ¸ ëª…: MLB ë°ì´í„°ë¥¼ í™œìš©í•œ íšŒê·€ëª¨ë¸ ìƒì„± ë° ê²°ê³¼ë¶„ì„

## ë°ì´í„° ì¶œì²˜: [Moneyball | Kaggle](https://www.kaggle.com/wduckett/moneyball-mlb-stats-19622012)

## í”„ë¡œì íŠ¸ ëª©í‘œ
    MLB Moneyball ë°ì´í„°ì™€ ê°•ì˜ ì‹¤ìŠµì‹œê°„ì— ë°°ìš´ ë‚´ìš©ìœ¼ë¡œ íšŒê·€ë¶„ì„ ë° ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ ëª¨ë¸ ìƒì„±
        - í•œ ì‹œì¦Œ ë™ì•ˆ ìŠ¹ë¦¬í•œ íšŸìˆ˜(W) ì˜ˆì¸¡ íšŒê·€ë¶„ì„ ëª¨ë¸, í”Œë ˆì´ì˜¤í”„ ì§„ì¶œ ì—¬ë¶€(Playoffs) ê²°ì • ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ ëª¨ë¸ ìƒì„±
    ë…ë¦½ë³€ìˆ˜ë“¤ê³¼ ì¢…ì†ë³€ìˆ˜ì™€ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì— ì˜í–¥ë ¥ì´ í° ìœ ì˜ë¯¸í•œ ë…ë¦½ë³€ìˆ˜ ì°¾ê¸°
        - ê¸°ì¡´ì˜ ë…ë¦½ë³€ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ ë§Œë“  ë³€ìˆ˜ë¡œ ì˜ˆì¸¡í•´ë³´ê¸°
        - ë³€ìˆ˜ì„ íƒë²•(ì „ì§„ì„ íƒë²•, í›„ì§„ì†Œê±°ë²•)ìœ¼ë¡œ ìµœì ì˜ ë³€ìˆ˜ ì¡°í•© ì°¾ê¸°
        - ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ í™•ì¸
    íšŒê·€ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ í•´ì„í•˜ëŠ” ë°©ë²• ìŠµë“

## í”„ë¡œì íŠ¸ êµ¬ì„±
    - ì‹œê°í™”ë¥¼ í†µí•œ ë°ì´í„°ì˜ ì´í•´
    - RSë¥¼ ë…ë¦½ë³€ìˆ˜ë¡œ Wë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‹¨ìˆœì„ í˜•íšŒê·€ ëª¨ë¸ ìƒì„±
    - (RS-RA)ë¥¼ ë…ë¦½ë³€ìˆ˜ë¡œ Wë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‹¨ìˆœì„ í˜•íšŒê·€ ëª¨ë¸ ìƒì„±
    - íšŒê·€ë¶„ì„ ê²°ê³¼ì˜ í•´ì„
    - ëª¨ë“  ë³€ìˆ˜ë¥¼ í™œìš©í•œ ë‹¤ì¤‘íšŒê·€ë¶„ì„ ë° ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ
    - ë¡œì§€ìŠ¤í‹±íšŒê·€ ëª¨ë¸ ìƒì„±
    - ë³€ìˆ˜ ì„ íƒë²•ìœ¼ë¡œ ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ ì •í™•ë„ ì˜¬ë¦¬ê¸°

## í”„ë¡œì íŠ¸ ê³¼ì •
    - ë°ì´í„°ì˜ ê°„ë‹¨í•œ ì‹œê°í™”ì—ì„œë¶€í„° íšŒê·€ë¶„ì„ê³¼ ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ ë¬¸ì œ í•´ê²°ê¹Œì§€ ê°•ì˜ ì‹¤ìŠµ ë‚´ìš© í™•ì¸
    - ëª¨ë¸ ìƒì„± ë° í•´ì„ì— ëŒ€í•œ ë‚´ìš©ì— ì§‘ì¤‘í•˜ê¸° ìœ„í•´ì„œ í•™ìŠµë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ì§€ ì•Šê³  ì§„í–‰
    - ê°•ì˜ ì‹¤ìŠµ ì‹œê°„ì— ë‹¤ë£¬ ìë£Œë¥¼ ì´ìš©í•´ì„œ ì½”ë“œ ì‘ì„±

- ì‘ì„±ì: ê¹€ë¯¼ì¤‘ ê°ìˆ˜ì

---

### Context

In the early 2000s, Billy Beane and Paul DePodesta worked for the Oakland Athletics. While there, they literally changed the game of baseball. They didn't do it using a bat or glove, and they certainly didn't do it by throwing money at the issue; in fact, money was the issue. They didn't have enough of it, but they were still expected to keep up with teams that had much deeper pockets. This is where Statistics came riding down the hillside on a white horse to save the day. This data set contains some of the information that was available to Beane and DePodesta in the early 2000s, and it can be used to better understand their methods.

### Content

This data set contains a set of variables that Beane and DePodesta focused heavily on. They determined that stats like on-base percentage (OBP) and slugging percentage (SLG) were very important when it came to scoring runs, however they were largely undervalued by most scouts at the time. This translated to a gold mine for Beane and DePodesta. Since these players weren't being looked at by other teams, they could recruit these players on a small budget. The variables are as follows:

- Team, íŒ€
- League, ë¦¬ê·¸
- Year, ì—°ë„
- Runs Scored (RS), ë“ì  ìˆ˜
- Runs Allowed (RA), ì‹¤ì  ìˆ˜
- Wins (W), ìŠ¹ë¦¬ íšŸìˆ˜
- On-Base Percentage (OBP), ì¶œë£¨ìœ¨
- Slugging Percentage (SLG), ì¥íƒ€ìœ¨
- Batting Average (BA), íƒ€ìœ¨
- Playoffs (binary), í”Œë ˆì´ì˜¤í”„ ì§„ì¶œ ì—¬ë¶€
- RankSeason, ì‹œì¦Œ ìˆœìœ„
- RankPlayoffs í”Œë ˆì´ì˜¤í”„ ìˆœìœ„
- Games Played (G), ê²½ê¸° ìˆ˜
- Opponent On-Base Percentage (OOBP), ë„ë£¨ í—ˆìš©ë¥ 
- Opponent Slugging Percentage (OSLG), í”¼ì¥íƒ€ìœ¨

### [Baseball statistics](https://en.wikipedia.org/wiki/Baseball_statistics)ì„ ì‚´í´ë³´ë©´ ê° ë³€ìˆ˜ì˜ ì •í™•í•œ ì˜ë¯¸ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.

# ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.


```python
# ì°¸ê³ : í”„ë¡œì íŠ¸ ì¶œì œìì˜ Python ë° ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „

import pandas as pd 
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import sys

print("python version: ", sys.version)
print("pandas version: ", pd.__version__)
print("statsmodels version: ", sm.__version__)
print("numpy version: ", np.__version__)

%matplotlib inline
```

    python version:  3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]
    pandas version:  0.25.1
    statsmodels version:  0.10.1
    numpy version:  1.16.5
    


```python
# Kaggleì˜ ì •ì±…ìƒ í”„ë¡œì íŠ¸ ì°¸ì—¬ìëŠ” Kaggleì— ì§ì ‘ ë¡œê·¸ì¸í•´ì„œ ìë£Œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.
# moneyball = pd.read_csv("https://mjgim-fc.s3.ap-northeast-2.amazonaws.com/baseball.csv", encoding="utf8")

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°(ìë£ŒëŠ” data í´ë”ì— ìˆìŒ)
moneyball = pd.read_csv("./data/baseball.csv", encoding="utf8")
moneyball.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Team</th>
      <th>League</th>
      <th>Year</th>
      <th>RS</th>
      <th>RA</th>
      <th>W</th>
      <th>OBP</th>
      <th>SLG</th>
      <th>BA</th>
      <th>Playoffs</th>
      <th>RankSeason</th>
      <th>RankPlayoffs</th>
      <th>G</th>
      <th>OOBP</th>
      <th>OSLG</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>ARI</td>
      <td>NL</td>
      <td>2012</td>
      <td>734</td>
      <td>688</td>
      <td>81</td>
      <td>0.328</td>
      <td>0.418</td>
      <td>0.259</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>162</td>
      <td>0.317</td>
      <td>0.415</td>
    </tr>
    <tr>
      <td>1</td>
      <td>ATL</td>
      <td>NL</td>
      <td>2012</td>
      <td>700</td>
      <td>600</td>
      <td>94</td>
      <td>0.320</td>
      <td>0.389</td>
      <td>0.247</td>
      <td>1</td>
      <td>4.0</td>
      <td>5.0</td>
      <td>162</td>
      <td>0.306</td>
      <td>0.378</td>
    </tr>
    <tr>
      <td>2</td>
      <td>BAL</td>
      <td>AL</td>
      <td>2012</td>
      <td>712</td>
      <td>705</td>
      <td>93</td>
      <td>0.311</td>
      <td>0.417</td>
      <td>0.247</td>
      <td>1</td>
      <td>5.0</td>
      <td>4.0</td>
      <td>162</td>
      <td>0.315</td>
      <td>0.403</td>
    </tr>
    <tr>
      <td>3</td>
      <td>BOS</td>
      <td>AL</td>
      <td>2012</td>
      <td>734</td>
      <td>806</td>
      <td>69</td>
      <td>0.315</td>
      <td>0.415</td>
      <td>0.260</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>162</td>
      <td>0.331</td>
      <td>0.428</td>
    </tr>
    <tr>
      <td>4</td>
      <td>CHC</td>
      <td>NL</td>
      <td>2012</td>
      <td>613</td>
      <td>759</td>
      <td>61</td>
      <td>0.302</td>
      <td>0.378</td>
      <td>0.240</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>162</td>
      <td>0.335</td>
      <td>0.424</td>
    </tr>
  </tbody>
</table>
</div>




```python
# from google.colab import drive
# drive.mount('/content/drive')
```


```python
# ë°ì´í„°ì˜ ê°„ë‹¨í•œ ì •ë³´ íŒŒì•…(naì˜ ê°œìˆ˜ ë° ë°ì´í„° íƒ€ì…)
moneyball.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 1232 entries, 0 to 1231
    Data columns (total 15 columns):
    Team            1232 non-null object
    League          1232 non-null object
    Year            1232 non-null int64
    RS              1232 non-null int64
    RA              1232 non-null int64
    W               1232 non-null int64
    OBP             1232 non-null float64
    SLG             1232 non-null float64
    BA              1232 non-null float64
    Playoffs        1232 non-null int64
    RankSeason      244 non-null float64
    RankPlayoffs    244 non-null float64
    G               1232 non-null int64
    OOBP            420 non-null float64
    OSLG            420 non-null float64
    dtypes: float64(7), int64(6), object(2)
    memory usage: 144.5+ KB
    


```python
# naê°€ ìˆëŠ” ì»¬ëŸ¼ì¸ RankSeason, RankPlayoffsì„ ì œê±°(í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš© ì•ˆ í•¨)
moneyball = moneyball.dropna(axis=1)
moneyball.info()

# 1232ê°œì˜ objectì™€ 11ê°œì˜ ë³€ìˆ˜
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 1232 entries, 0 to 1231
    Data columns (total 11 columns):
    Team        1232 non-null object
    League      1232 non-null object
    Year        1232 non-null int64
    RS          1232 non-null int64
    RA          1232 non-null int64
    W           1232 non-null int64
    OBP         1232 non-null float64
    SLG         1232 non-null float64
    BA          1232 non-null float64
    Playoffs    1232 non-null int64
    G           1232 non-null int64
    dtypes: float64(3), int64(6), object(2)
    memory usage: 106.0+ KB
    


```python
# Object ë³€ìˆ˜ ë° ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±°í•´ì„œ ë‹¨ìˆœ ì‹œê°í™”

selected_df = moneyball.select_dtypes(exclude=['object'])
selected_df = selected_df.drop(["Year"], axis=1)
_ = pd.plotting.scatter_matrix(selected_df, alpha = 0.5, figsize=(16,12))
```

    C:\Users\drogpard\Anaconda3\lib\site-packages\pandas\plotting\_matplotlib\tools.py:307: MatplotlibDeprecationWarning: 
    The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.
      layout[ax.rowNum, ax.colNum] = ax.get_visible()
    C:\Users\drogpard\Anaconda3\lib\site-packages\pandas\plotting\_matplotlib\tools.py:307: MatplotlibDeprecationWarning: 
    The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.
      layout[ax.rowNum, ax.colNum] = ax.get_visible()
    C:\Users\drogpard\Anaconda3\lib\site-packages\pandas\plotting\_matplotlib\tools.py:313: MatplotlibDeprecationWarning: 
    The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.
      if not layout[ax.rowNum + 1, ax.colNum]:
    C:\Users\drogpard\Anaconda3\lib\site-packages\pandas\plotting\_matplotlib\tools.py:313: MatplotlibDeprecationWarning: 
    The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.
      if not layout[ax.rowNum + 1, ax.colNum]:
    


    
![png](/assets/images/output_8_1.png)
    


## STEP 1. ì‹œê°í™”ë¥¼ í†µí•œ ë°ì´í„°ì˜ ì´í•´ 
- ë‹¤ì–‘í•œ ìˆ˜ì¹˜ê°’ì„ ê°–ëŠ” ë³€ìˆ˜ë“¤ì˜ ì‚°í¬ë„ë¥¼ ë³´ê³  ë°›ì€ í†µì°°(insight)ì€ ë¬´ì—‡ì¸ê°€?
- ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ë°ì´í„°ë“¤ì€ ì¡´ì¬í•˜ëŠ”ê°€? ìˆë‹¤ë©´ ì–´ë–¤ ë³€ìˆ˜ë“¤ì´ ì–´ë–¤ ê´€ê³„ì— ìˆëŠ”ì§€ ëŒ€ë‹µí•˜ì‹œì˜¤.
- ê²½ê¸° ìˆ˜(G)ì˜ íˆìŠ¤í† ê·¸ë¨ì€ ì–´ë–¤ ì˜ë¯¸ë¥¼ ê°–ëŠ”ê°€? ë˜í•œ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì˜ íˆìŠ¤í† ê·¸ë¨ì„ ë³´ê³  í•´ì„í•˜ì‹œì˜¤.

### [í’€ì´] 
- RS ì™€ OBP, SLG, BA ë“¤ê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ê²ƒìœ¼ë¡œ ë³´ì„
- OBP, SLG, BA ë³€ìˆ˜ë“¤ë„ ì„œë¡œ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆìŒ
- ê²½ê¸° ìˆ˜(G) ëŠ” ëŒ€ë¶€ë¶„ 162ë¡œ í° ì˜ë¯¸ê°€ ì—†ë‹¤.

## STEP 2. RSë¥¼ ë…ë¦½ë³€ìˆ˜ë¡œ Wë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‹¨ìˆœì„ í˜•íšŒê·€ ëª¨ë¸ ìƒì„±
- ê°€ì •: ì‹œì¦Œ ì´ ë“ì (RS)ì´ ìŠ¹ë¦¬ì— ì˜í–¥ì„ ì¤„ ê²ƒì´ë‹¤.
- RSë¥¼ ë…ë¦½ë³€ìˆ˜ë¡œ Wë¥¼ ì˜ˆì¸¡í•œ ë‹¨ìˆœì„ í˜•ëª¨ë¸ ì§ì„ ì˜ ê¸°ìš¸ê¸° $\alpha$ì™€ ì ˆí¸ $\beta$ëŠ” ëª‡ì¸ê°€?

$$
W = \alpha RS + \beta
$$


- í•´ë‹¹ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì í•©í•œì§€ë¥¼ í‰ê°€í•˜ëŠ” $R^2$ëŠ” ëª‡ ì¸ê°€?
- ì ë‹¹í•œ ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ”ê°€?

### [í’€ì´] 
- ğ›¼ = 0.0641, ğ›½ = 35.0964
- R-squared = 0.262
- $R^2$ ì´ 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì„ í˜•íšŒê·€ ëª¨í˜•ì˜ ì„¤ëª…ë ¥ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ëœ»í•˜ë©° 0.262 ëŠ” ì ë‹¹í•œ ëª¨ë¸ì´ë¼ í•  ìˆ˜ ì—†ë‹¤.


```python
moneyball.plot.scatter(x = "RS", y = "W", alpha = 0.7)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2e94271a608>




    
![png](/assets/images/output_13_1.png)
    



```python
w = moneyball['W']
rs = moneyball['RS']
```


```python
rs1 = sm.add_constant(rs, has_constant="add")
rs1
```

    C:\Users\drogpard\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.
      return ptp(axis=axis, out=out, **kwargs)
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>const</th>
      <th>RS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.0</td>
      <td>734</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.0</td>
      <td>700</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.0</td>
      <td>712</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.0</td>
      <td>734</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.0</td>
      <td>613</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>1227</td>
      <td>1.0</td>
      <td>705</td>
    </tr>
    <tr>
      <td>1228</td>
      <td>1.0</td>
      <td>706</td>
    </tr>
    <tr>
      <td>1229</td>
      <td>1.0</td>
      <td>878</td>
    </tr>
    <tr>
      <td>1230</td>
      <td>1.0</td>
      <td>774</td>
    </tr>
    <tr>
      <td>1231</td>
      <td>1.0</td>
      <td>599</td>
    </tr>
  </tbody>
</table>
<p>1232 rows Ã— 2 columns</p>
</div>




```python
# smìœ¼ë¡œ fití•œ ëª¨ë¸ëª…ì€ fit_simple_model ìœ¼ë¡œ í•˜ì‹œì˜¤.
model1 = sm.OLS(w, rs1)
fit_simple_model = model1.fit()
```


```python
fit_simple_model.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>W</td>        <th>  R-squared:         </th> <td>   0.262</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.261</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   436.4</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 28 Sep 2020</td> <th>  Prob (F-statistic):</th> <td>3.50e-83</td>
</tr>
<tr>
  <th>Time:</th>                 <td>16:49:55</td>     <th>  Log-Likelihood:    </th> <td> -4565.1</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1232</td>      <th>  AIC:               </th> <td>   9134.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  1230</td>      <th>  BIC:               </th> <td>   9144.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   35.0964</td> <td>    2.211</td> <td>   15.876</td> <td> 0.000</td> <td>   30.759</td> <td>   39.434</td>
</tr>
<tr>
  <th>RS</th>    <td>    0.0641</td> <td>    0.003</td> <td>   20.890</td> <td> 0.000</td> <td>    0.058</td> <td>    0.070</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>14.041</td> <th>  Durbin-Watson:     </th> <td>   1.869</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  10.799</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.134</td> <th>  Prob(JB):          </th> <td> 0.00452</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.627</td> <th>  Cond. No.          </th> <td>5.68e+03</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.68e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.




```python
fit_simple_model.params
```




    const    35.096418
    RS        0.064060
    dtype: float64




```python
# ì°¸ê³ 
# Fitëœ ì§ì„  ê·¸ë¦¬ê¸°. smì„ ì´ìš©í•´ì„œ ì„ í˜•íšŒê·€ë¶„ì„ì„ í•œ ê²½ìš° 
# ì•„ë˜ì™€ ê°™ì´ ê°„ë‹¨í•œ ì½”ë“œë¡œ ì í•©ëœ ì§ì„ ê³¼ ì›ë˜ ë°ì´í„°ì˜ ê·¸ë¦¼ì„ ê·¸ë¦´ ìˆ˜ ìˆìŒ.

fig, ax = plt.subplots()
fig = sm.graphics.plot_fit(fit_simple_model, 1, ax=ax)
ax.set_ylabel("W")
ax.set_xlabel("RS")
ax.set_title("Linear Regression")
```




    Text(0.5, 1.0, 'Linear Regression')




    
![png](/assets/images/output_19_1.png)
    


### Comments
- ë“ì  RSë¡œ Wë¥¼ ì˜ˆì¸¡í•œ ë‹¨ìˆœì„ í˜•íšŒê·€ë¶„ì„ì˜ ì í•©ë„ëŠ” ë§Œì¡±í•˜ê¸° ì–´ë ¤ì›€(R squared ê°’ìœ¼ë¡œ íŒë‹¨).
- ì‹œì¦Œ ì´ ì‹¤ì  RAë¥¼ ë…ë¦½ë³€ìˆ˜ë¡œ Wë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‹¨ìˆœì„ í˜•íšŒê·€ë¶„ì„ì˜ ê²°ê³¼ëŠ” ì–´ë– í•œê°€? ë§Œì¡±í•  ë§Œí•œê°€? 
- RSë¥¼ RAë¡œ ë³€ê²½í•´ì„œ ê¸°ìš¸ê¸°ì™€ ì ˆí¸ ë° R squaredë¥¼ êµ¬í•´ë³´ì‹œì˜¤. 

### [í’€ì´] 
- ğ›¼ = -0.0655, ğ›½ = 127.7690
- R-squared = 0.283
- $R^2$ ì´ 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì„ í˜•íšŒê·€ ëª¨í˜•ì˜ ì„¤ëª…ë ¥ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ëœ»í•˜ë©° 0.283 ì—­ì‹œ ì ë‹¹í•œ ëª¨ë¸ì´ë¼ í•  ìˆ˜ ì—†ë‹¤.


```python
ra = moneyball['RA']
```


```python
ra1 = sm.add_constant(ra, has_constant="add")
ra1
```

    C:\Users\drogpard\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.
      return ptp(axis=axis, out=out, **kwargs)
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>const</th>
      <th>RA</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.0</td>
      <td>688</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.0</td>
      <td>600</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.0</td>
      <td>705</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.0</td>
      <td>806</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.0</td>
      <td>759</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>1227</td>
      <td>1.0</td>
      <td>759</td>
    </tr>
    <tr>
      <td>1228</td>
      <td>1.0</td>
      <td>626</td>
    </tr>
    <tr>
      <td>1229</td>
      <td>1.0</td>
      <td>690</td>
    </tr>
    <tr>
      <td>1230</td>
      <td>1.0</td>
      <td>664</td>
    </tr>
    <tr>
      <td>1231</td>
      <td>1.0</td>
      <td>716</td>
    </tr>
  </tbody>
</table>
<p>1232 rows Ã— 2 columns</p>
</div>




```python
model2 = sm.OLS(w, ra1)
fit_simple_model2 = model2.fit()
```


```python
fit_simple_model2.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>W</td>        <th>  R-squared:         </th> <td>   0.283</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.283</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   486.5</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 28 Sep 2020</td> <th>  Prob (F-statistic):</th> <td>4.06e-91</td>
</tr>
<tr>
  <th>Time:</th>                 <td>16:49:56</td>     <th>  Log-Likelihood:    </th> <td> -4546.8</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1232</td>      <th>  AIC:               </th> <td>   9098.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  1230</td>      <th>  BIC:               </th> <td>   9108.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>  127.7690</td> <td>    2.143</td> <td>   59.634</td> <td> 0.000</td> <td>  123.566</td> <td>  131.973</td>
</tr>
<tr>
  <th>RA</th>    <td>   -0.0655</td> <td>    0.003</td> <td>  -22.058</td> <td> 0.000</td> <td>   -0.071</td> <td>   -0.060</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 5.903</td> <th>  Durbin-Watson:     </th> <td>   1.843</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.052</td> <th>  Jarque-Bera (JB):  </th> <td>   5.615</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.127</td> <th>  Prob(JB):          </th> <td>  0.0604</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.789</td> <th>  Cond. No.          </th> <td>5.59e+03</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.59e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.




```python
fit_simple_model2.params
```




    const    127.769033
    RA        -0.065538
    dtype: float64




```python
fig, ax = plt.subplots()
fig = sm.graphics.plot_fit(fit_simple_model2, 1, ax=ax)
ax.set_ylabel("W")
ax.set_xlabel("RA")
ax.set_title("Linear Regression")
```




    Text(0.5, 1.0, 'Linear Regression')




    
![png](/assets/images/output_27_1.png)
    


## STEP 3. (RS-RA)ë¥¼ ë…ë¦½ë³€ìˆ˜ë¡œ Wë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‹¨ìˆœì„ í˜•íšŒê·€ ëª¨ë¸ ìƒì„±
- ë“ì ê³¼ ì‹¤ì ì´ ìŠ¹ë¦¬ ìˆ˜ì™€ ê´€ë ¨ì´ ì—†ì„ê¹Œ? ê²½ê¸°ì—ì„œ ìŠ¹ë¦¬í•˜ë ¤ë©´ ìƒëŒ€ë³´ë‹¤ ë“ì ì„ ë§ì´ í•´ì•¼í•œë‹¤. ì¦‰, (ë“ì  - ì‹¤ì )ì„ ìƒˆë¡œìš´ ë…ë¦½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ê³  Wë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‹¨ìˆœì„ í˜•íšŒê·€ë¶„ì„ì„ í•´ë³´ì‹œì˜¤.
- ê°€ì •: ì‹œì¦Œ ì´ ë“ì (RS)ê³¼ ì‹¤ì (RA)ì˜ ì°¨ì´ê°€ ìŠ¹ë¦¬ì— ì˜í–¥ì„ ì¤„ ê²ƒì´ë‹¤.
- ê°•ì˜ ì‹¤ìŠµ ì‹œê°„ì— í•™ìŠµí•œ statsmodels ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•´ì„œ ì•„ë˜ì˜ ì§ˆë¬¸ì— ë‹µí•˜ì‹œì˜¤.
- (RS-RA)ë¥¼ ë…ë¦½ë³€ìˆ˜ë¡œ Wë¥¼ ì˜ˆì¸¡í•œ ë‹¨ìˆœì„ í˜•ëª¨ë¸ì˜ ê¸°ìš¸ê¸° $\alpha$ì™€ ì ˆí¸ $\beta$ëŠ” ëª‡ì¸ê°€?

$$
W = \alpha \cdot (RS-RA) + \beta
$$

- í•´ë‹¹ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì í•©í•œì§€ë¥¼ í‰ê°€í•˜ëŠ” $R^2$ëŠ” ëª‡ ì¸ê°€?
- ì ë‹¹í•œ ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ”ê°€?

### [í’€ì´] 
- ğ›¼ = 0.1045, ğ›½ = 80.9042
- R-squared = 0.880
- $R^2$ ê°€ 0.880 ì´ë¯€ë¡œ RS, RA ë¥¼ ê°ê° ë…ë¦½ë³€ìˆ˜ë¡œ ì˜ˆì¸¡í•œ ëª¨ë¸ ë³´ë‹¤ í›¨ì”¬ ì ë‹¹í•œ ëª¨ë¸ì´ë¼ í•  ìˆ˜ ìˆë‹¤.


```python
# rd = moneyball['RS'] - moneyball['RA']
rd = pd.DataFrame(moneyball['RS'] - moneyball['RA'], columns = ['RD'])
```


```python
rd1 = sm.add_constant(rd, has_constant="add")
rd1.head()
```

    C:\Users\drogpard\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.
      return ptp(axis=axis, out=out, **kwargs)
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>const</th>
      <th>RD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.0</td>
      <td>46</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.0</td>
      <td>100</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.0</td>
      <td>7</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.0</td>
      <td>-72</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.0</td>
      <td>-146</td>
    </tr>
  </tbody>
</table>
</div>




```python
model3 = sm.OLS(w, rd1)
fit_simple_model3 = model3.fit()
```


```python
fit_simple_model3.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>W</td>        <th>  R-squared:         </th> <td>   0.880</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.879</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   8983.</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 28 Sep 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>16:49:56</td>     <th>  Log-Likelihood:    </th> <td> -3448.3</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1232</td>      <th>  AIC:               </th> <td>   6901.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  1230</td>      <th>  BIC:               </th> <td>   6911.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   80.9042</td> <td>    0.113</td> <td>  713.853</td> <td> 0.000</td> <td>   80.682</td> <td>   81.127</td>
</tr>
<tr>
  <th>RD</th>    <td>    0.1045</td> <td>    0.001</td> <td>   94.778</td> <td> 0.000</td> <td>    0.102</td> <td>    0.107</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.797</td> <th>  Durbin-Watson:     </th> <td>   2.065</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.671</td> <th>  Jarque-Bera (JB):  </th> <td>   0.686</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.041</td> <th>  Prob(JB):          </th> <td>   0.710</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.081</td> <th>  Cond. No.          </th> <td>    103.</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.




```python
fit_simple_model3.params
```




    const    80.904221
    RD        0.104548
    dtype: float64




```python
fig, ax = plt.subplots()
fig = sm.graphics.plot_fit(fit_simple_model3, 1, ax=ax)
ax.set_ylabel("W")
ax.set_xlabel("RD")
ax.set_title("Linear Regression")
```




    Text(0.5, 1.0, 'Linear Regression')




    
![png](/assets/images/output_35_1.png)
    



```python
print(fit_simple_model.params)
print(fit_simple_model2.params)
print(fit_simple_model3.params) ##ë‹¨ìˆœì„ í˜•íšŒê·€ë¶„ì„ì˜ íšŒê·€ ê³„ìˆ˜(R2)ì™€ ë¹„êµ
```

    const    35.096418
    RS        0.064060
    dtype: float64
    const    127.769033
    RA        -0.065538
    dtype: float64
    const    80.904221
    RD        0.104548
    dtype: float64
    


```python
fit_simple_model.resid.plot(label="rs")
fit_simple_model2.resid.plot(label="ra")
fit_simple_model3.resid.plot(label="rd")
plt.legend()
```




    <matplotlib.legend.Legend at 0x2e9462a47c8>




    
![png](/assets/images/output_37_1.png)
    


## STEP 4. íšŒê·€ë¶„ì„ ê²°ê³¼ì˜ í•´ì„
- ë“ì ê³¼ ì‹¤ì ì´ ê°œë³„ì ìœ¼ë¡œ í•œ ê°œì”©ë§Œ ë³¸ë‹¤ë©´ ìŠ¹ë¦¬ ì˜ˆì¸¡ì— í° ì˜í–¥ì„ ì£¼ì§€ ëª»í•˜ì§€ë§Œ (ë“ì  - ì‹¤ì )ìœ¼ë¡œ ê²°í•©í•œ ë³€ìˆ˜ëŠ” ìŠ¹ë¦¬ ì˜ˆì¸¡ì— ìœ ì˜ë¯¸í•˜ê²Œ ì˜í–¥ì„ ì£¼ì—ˆë‹¤. ì´ëŸ° ì‘ìš©ì€ ë¬´ì—‡ì´ë¼ê³  í•˜ëŠ”ê°€?
- ë‹¤ì‹œ RS, RAë¥¼ ë‘ ê°œì˜ ë…ë¦½ë³€ìˆ˜ë¡œ Wë¥¼ ì˜ˆì¸¡í•œ ì„ í˜•ëª¨ë¸ì˜ ê¸°ìš¸ê¸° $\alpha_1$, $\alpha_2$ì™€ ì ˆí¸ $\beta$ëŠ” ëª‡ì¸ê°€?

$$
W = \alpha_1 RS + \alpha_2 RA + \beta
$$

- RS, RAë¥¼ ë‘ ê°œì˜ ë…ë¦½ë³€ìˆ˜ë¡œ Wë¥¼ ì˜ˆì¸¡í•œ ëª¨ë¸ê³¼ (RS-RA)ì„ ë…ë¦½ë³€ìˆ˜ë¡œ Wë¥¼ ì˜ˆì¸¡í•œ ëª¨ë¸ì€ ë¬´ìŠ¨ ì°¨ì´ê°€ ìˆì„ê¹Œ?
- íšŒê·€ë¶„ì„ ê²°ê³¼ì¸ $\alpha_1$, $\alpha_2$ì™€ (RS-RA)ì˜ ê³„ìˆ˜ì¸ $\alpha$ë¥¼ ë¹„êµí•´ë³´ê³  RSì™€ RAì˜ ì°¨ì´ê°€ ìŠ¹ë¦¬ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤ê³  ê²°ë¡  ë‚´ë¦´ ìˆ˜ ìˆëŠ”ê°€?

### [í’€ì´] 
- êµí˜¸ì‘ìš©(Interaction term)
- ğ›¼1 = 0.1045 , ğ›¼2 = -0.1046, ğ›½ = 80.9805
- R-squared = 0.880


```python
x_data = moneyball[['RS', 'RA']]
x_data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RS</th>
      <th>RA</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>734</td>
      <td>688</td>
    </tr>
    <tr>
      <td>1</td>
      <td>700</td>
      <td>600</td>
    </tr>
    <tr>
      <td>2</td>
      <td>712</td>
      <td>705</td>
    </tr>
    <tr>
      <td>3</td>
      <td>734</td>
      <td>806</td>
    </tr>
    <tr>
      <td>4</td>
      <td>613</td>
      <td>759</td>
    </tr>
  </tbody>
</table>
</div>




```python
#ìƒìˆ˜í•­ ì¶”ê¸°
x_data1 = sm.add_constant(x_data, has_constant="add")
```

    C:\Users\drogpard\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.
      return ptp(axis=axis, out=out, **kwargs)
    


```python
# íšŒê·€ëª¨ë¸ ì í•©
multi_model = sm.OLS(w, x_data1)
fitted_multi_model = multi_model.fit()
```


```python
# summaryí•¨ìˆ˜ë¥¼ í†µí•´ ê²°ê³¼ì¶œë ¥ 
fitted_multi_model.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>W</td>        <th>  R-squared:         </th> <td>   0.880</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.879</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4488.</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 28 Sep 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>16:49:57</td>     <th>  Log-Likelihood:    </th> <td> -3448.3</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1232</td>      <th>  AIC:               </th> <td>   6903.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  1229</td>      <th>  BIC:               </th> <td>   6918.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   80.9805</td> <td>    1.064</td> <td>   76.111</td> <td> 0.000</td> <td>   78.893</td> <td>   83.068</td>
</tr>
<tr>
  <th>RS</th>    <td>    0.1045</td> <td>    0.001</td> <td>   77.995</td> <td> 0.000</td> <td>    0.102</td> <td>    0.107</td>
</tr>
<tr>
  <th>RA</th>    <td>   -0.1046</td> <td>    0.001</td> <td>  -79.393</td> <td> 0.000</td> <td>   -0.107</td> <td>   -0.102</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.802</td> <th>  Durbin-Watson:     </th> <td>   2.065</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.670</td> <th>  Jarque-Bera (JB):  </th> <td>   0.691</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.041</td> <th>  Prob(JB):          </th> <td>   0.708</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.081</td> <th>  Cond. No.          </th> <td>9.54e+03</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 9.54e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.




```python
fitted_multi_model.params
```




    const    80.980456
    RS        0.104493
    RA       -0.104600
    dtype: float64




```python
from numpy import linalg ##í–‰ë ¬ì—°ì‚°ì„ í†µí•´ betaêµ¬í•˜ê¸° 
ba = linalg.inv((np.dot(x_data1.T, x_data1))) ## (X'X)-1
np.dot(np.dot(ba,x_data1.T), w) ##(X'X)-1X'y
```




    array([80.98045556,  0.10449347, -0.10460008])




```python
pred4 = fitted_multi_model.predict(x_data1)
```


```python
fitted_multi_model.resid.plot()
plt.xlabel("residual_number")
plt.show()
```


    
![png](/assets/images/output_47_0.png)
    



```python
fit_simple_model.resid.plot(label="rs")
fit_simple_model2.resid.plot(label="ra")
fit_simple_model3.resid.plot(label="rd")
fitted_multi_model.resid.plot(label="ra, rs")
plt.legend()
```




    <matplotlib.legend.Legend at 0x2e9465fc508>




    
![png](/assets/images/output_48_1.png)
    


## STEP 5. ëª¨ë“  ë³€ìˆ˜ë¥¼ í™œìš©í•œ ë‹¤ì¤‘íšŒê·€ë¶„ì„ ë° ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ
- [RS, RA, OBP, SLG, BA, G] 6ê°œ ë…ë¦½ë³€ìˆ˜ë¡œ Wë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‹¤ì¤‘íšŒê·€ë¶„ì„ í•˜ì‹œì˜¤.
- RS, RA ë‘ ë…ë¦½ë³€ìˆ˜ì˜ ëª¨ë¸ê³¼ ë¹„êµí–ˆì„ ë•Œ ê²°ê³¼ê°€ í–¥ìƒëë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ê°€?
- ê²°ê³¼ê°€ í–¥ìƒë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆëŠ”ë° ì •ëŸ‰ì ì¸ ìˆ˜ì¹˜ë¡œ í™•ì¸í•´ë³´ì‹œì˜¤. VIFë¥¼ êµ¬í•˜ê³  ê²°ê³¼ë¥¼ í•´ì„í•˜ì‹œì˜¤(íŠ¹ë³„íˆ RSì˜ VIF ìˆ˜ì¹˜ê°€ ë†’ê²Œ ë‚˜ì˜¨ ì´ìœ ë¥¼ í•œë²ˆ ìƒê°í•´ ë³´ì‹œì˜¤).
    - RS, RA ë‘ ë³€ìˆ˜ë¡œë„ ì¶©ë¶„íˆ Wë¥¼ ì„¤ëª… ê°€ëŠ¥í–ˆì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ì–´ë–»ê²Œ í•´ì„í•´ì•¼ í• ê¹Œìš”?
    - Gì˜ VIF ìˆ˜ì¹˜ëŠ” ì–´ë–»ê²Œ í•´ì„í•´ì•¼ í• ê¹Œìš”? ê°€ì¥ ë‚®ì€ VIFê°€ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ë³€ìˆ˜ì¸ê°€ìš”?
- ì£¼ì˜ statsmodelsì—ëŠ” í•­ìƒ ìƒìˆ˜í•­ì„ ë„£ì–´ì£¼ì–´ì•¼ í•œë‹¤. VIFë„ ë§ˆì°¬ê°€ì§€ì„. [ì°¸ê³ ](https://stackoverflow.com/questions/42658379/variance-inflation-factor-in-python)
- ê°•ì˜ ì‹¤ìŠµ ë•Œ ë‹¤ë£¬ ì „ì§„ì„ íƒë²•ì„ ì‚¬ìš©í•´ì„œ AICê°€ ê°€ì¥ ì‘ì€ ê°’ì´ ë‚˜ì˜¤ëŠ” ë…ë¦½ë³€ìˆ˜ë¥¼ ì°¾ìœ¼ì‹œì˜¤.

### [í’€ì´] 
- R-squared ê°€ 0.881 ë¡œ ë¹„ìŠ·í•˜ë©° ë³€ìˆ˜ê°€ ì¦ê°€í•˜ë©´ SSRì´ ì¦ê°€í•˜ê³  ğ‘…2 ë˜í•œ ì¦ê°€í•˜ê¸° ë•Œë¬´ì— ê²°ê³¼ê°€ í–¥ìƒ ë˜ì—ˆë‹¤ê³  ë§í•˜ê¸° ì–´ë µë‹¤.
- Wë¥¼ ì„¤ëª… í•˜ëŠ”ë° ì—¬ëŸ¬ ë³€ìˆ˜ë“¤ì´ ë³€ë™ì„±ì´ ê²¹ì¹˜ë©° ê²¹ì¹˜ëŠ” ë³€ë™ì„±ì— ëŒ€í•´ì„œëŠ” ì¤‘ë³µìœ¼ë¡œ ê°€ì ¸ê°€ì§€ ëª»í•œë‹¤.
- VIF ë¥¼ ë³´ë©´ OBP, SLG, BA ì´ ë³€ìˆ˜ë“¤ì´ ë¹„êµì  ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆìœ¼ë©° RS ë³€ìˆ˜ì™€ ì„œë¡œ ë†’ì€ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
- ë”°ë¼ì„œ ì¶œë£¨ìœ¨(OBP), ì¥íƒ€ìœ¨(SLG), íƒ€ìœ¨(BA) ì´ ë†’ë‹¤ëŠ” ê²ƒì€ ë†’ì€ ë“ì ìˆ˜(RS) ë¡œ ì´ì–´ì§„ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.
- ì´ ì™€ ê°™ì´ ë³€ë™ì„±ì´ ê²¹ì¹˜ëŠ” ë³€ìˆ˜ë“¤ì„ ì œê±°í•˜ê³  RS, RA ë‘ ë³€ìˆ˜ë¡œë„ ì¶©ë¶„íˆ Wì˜ ë³€ë™ì„±ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.
- ê²½ê¸° ìˆ˜(G) ëŠ” ëŒ€ë¶€ë¶„ 162ë¡œ í° ì˜ë¯¸ê°€ ì—†ìœ¼ë©° VIF ê°€ ê°€ì¥ ë‚®ë‹¤ê³  í•˜ì—¬ ì¤‘ìš”í•œ ë³€ìˆ˜ëŠ” ì•„ë‹˜


```python
x_data_S5 = moneyball[['RS','RA','OBP','SLG','BA','G']] ##ë³€ìˆ˜ ì—¬ëŸ¬ê°œ
x_data_S5.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RS</th>
      <th>RA</th>
      <th>OBP</th>
      <th>SLG</th>
      <th>BA</th>
      <th>G</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>734</td>
      <td>688</td>
      <td>0.328</td>
      <td>0.418</td>
      <td>0.259</td>
      <td>162</td>
    </tr>
    <tr>
      <td>1</td>
      <td>700</td>
      <td>600</td>
      <td>0.320</td>
      <td>0.389</td>
      <td>0.247</td>
      <td>162</td>
    </tr>
    <tr>
      <td>2</td>
      <td>712</td>
      <td>705</td>
      <td>0.311</td>
      <td>0.417</td>
      <td>0.247</td>
      <td>162</td>
    </tr>
    <tr>
      <td>3</td>
      <td>734</td>
      <td>806</td>
      <td>0.315</td>
      <td>0.415</td>
      <td>0.260</td>
      <td>162</td>
    </tr>
    <tr>
      <td>4</td>
      <td>613</td>
      <td>759</td>
      <td>0.302</td>
      <td>0.378</td>
      <td>0.240</td>
      <td>162</td>
    </tr>
  </tbody>
</table>
</div>




```python
x_data_S5_2 = sm.add_constant(x_data_S5, has_constant='add')
```

    C:\Users\drogpard\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.
      return ptp(axis=axis, out=out, **kwargs)
    


```python
multi_model_S5 = sm.OLS(w,x_data_S5_2)
fitted_multi_model_S5 = multi_model_S5.fit()
```


```python
fitted_multi_model_S5.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>W</td>        <th>  R-squared:         </th> <td>   0.881</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.881</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1514.</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 28 Sep 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>16:49:57</td>     <th>  Log-Likelihood:    </th> <td> -3440.1</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1232</td>      <th>  AIC:               </th> <td>   6894.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  1225</td>      <th>  BIC:               </th> <td>   6930.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>  -12.0766</td> <td>   30.741</td> <td>   -0.393</td> <td> 0.695</td> <td>  -72.388</td> <td>   48.235</td>
</tr>
<tr>
  <th>RS</th>    <td>    0.0912</td> <td>    0.005</td> <td>   19.957</td> <td> 0.000</td> <td>    0.082</td> <td>    0.100</td>
</tr>
<tr>
  <th>RA</th>    <td>   -0.1050</td> <td>    0.001</td> <td>  -77.700</td> <td> 0.000</td> <td>   -0.108</td> <td>   -0.102</td>
</tr>
<tr>
  <th>OBP</th>   <td>   58.5427</td> <td>   20.365</td> <td>    2.875</td> <td> 0.004</td> <td>   18.588</td> <td>   98.497</td>
</tr>
<tr>
  <th>SLG</th>   <td>   22.9386</td> <td>    9.409</td> <td>    2.438</td> <td> 0.015</td> <td>    4.480</td> <td>   41.397</td>
</tr>
<tr>
  <th>BA</th>    <td>  -27.0409</td> <td>   17.942</td> <td>   -1.507</td> <td> 0.132</td> <td>  -62.241</td> <td>    8.159</td>
</tr>
<tr>
  <th>G</th>     <td>    0.5040</td> <td>    0.184</td> <td>    2.742</td> <td> 0.006</td> <td>    0.143</td> <td>    0.865</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.385</td> <th>  Durbin-Watson:     </th> <td>   2.055</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.825</td> <th>  Jarque-Bera (JB):  </th> <td>   0.297</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.025</td> <th>  Prob(JB):          </th> <td>   0.862</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.057</td> <th>  Cond. No.          </th> <td>2.87e+05</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.87e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems.




```python
## ìƒê´€í–‰ë ¬ ì‹œê°í™” í•´ì„œ ë³´ê¸° 
import seaborn as sns;
cmap = sns.light_palette("darkgray", as_cmap=True)
sns.heatmap(x_data_S5_2.corr(), annot=True, cmap=cmap)
plt.show()
```


    
![png](output_55_0.png)
    



```python
## ë³€ìˆ˜ë³„ ì‚°ì ë„ ì‹œê°í™”
sns.pairplot(x_data_S5_2)
plt.show()
```


    
![png](/assets/images/output_56_0.png)
    



```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x_data_S5_2.values, i) for i in range(x_data_S5_2.shape[1])]
vif["features"] = x_data_S5_2.columns
vif
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>VIF Factor</th>
      <th>features</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>74248.579971</td>
      <td>const</td>
    </tr>
    <tr>
      <td>1</td>
      <td>13.745013</td>
      <td>RS</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.241633</td>
      <td>RA</td>
    </tr>
    <tr>
      <td>3</td>
      <td>7.338020</td>
      <td>OBP</td>
    </tr>
    <tr>
      <td>4</td>
      <td>7.690644</td>
      <td>SLG</td>
    </tr>
    <tr>
      <td>5</td>
      <td>4.209910</td>
      <td>BA</td>
    </tr>
    <tr>
      <td>6</td>
      <td>1.033843</td>
      <td>G</td>
    </tr>
  </tbody>
</table>
</div>




```python
def processSubset(X,y, feature_set):
            model = sm.OLS(y,X[list(feature_set)]) # Modeling
            regr = model.fit() # ëª¨ë¸ í•™ìŠµ
            AIC = regr.aic # ëª¨ë¸ì˜ AIC
            return {"model":regr, "AIC":AIC}

# feature_columns = list(x_data_S5_2.columns)
# print(processSubset(X=x_data_S5_2, y=w, feature_set = feature_columns[0:7]))
```


```python
import time
########ì „ì§„ì„ íƒë²•(step=1)

def forward(X, y, predictors):
    # ë°ì´í„° ë³€ìˆ˜ë“¤ì´ ë¯¸ë¦¬ì •ì˜ëœ predictorsì— ìˆëŠ”ì§€ ì—†ëŠ”ì§€ í™•ì¸ ë° ë¶„ë¥˜
    remaining_predictors = [p for p in X.columns.difference(['const']) if p not in predictors]
    tic = time.time()
    results = []
    for p in remaining_predictors:
        results.append(processSubset(X=X, y= y, feature_set=predictors+[p]+['const']))
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    models = pd.DataFrame(results)

    # AICê°€ ê°€ì¥ ë‚®ì€ ê²ƒì„ ì„ íƒ
    best_model = models.loc[models['AIC'].argmin()] # index
    toc = time.time()
    print("Processed ", models.shape[0], "models on", len(predictors)+1, "predictors in", (toc-tic))
    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )
    return best_model
```


```python
#### ì „ì§„ì„ íƒë²• ëª¨ë¸

def forward_model(X,y):
    Fmodels = pd.DataFrame(columns=["AIC", "model"])
    tic = time.time()
    # ë¯¸ë¦¬ ì •ì˜ëœ ë°ì´í„° ë³€ìˆ˜
    predictors = []
    # ë³€ìˆ˜ 1~10ê°œ : 0~9 -> 1~10
    for i in range(1, len(X.columns.difference(['const'])) + 1):
        Forward_result = forward(X=X,y=y,predictors=predictors)
        if i > 1:
            if Forward_result['AIC'] > Fmodel_before:
                break
        Fmodels.loc[i] = Forward_result
        predictors = Fmodels.loc[i]["model"].model.exog_names
        Fmodel_before = Fmodels.loc[i]["AIC"]
        print('AIC:',Forward_result['AIC'])
        predictors = [ k for k in predictors if k != 'const']
    toc = time.time()
    print("Total elapsed time:", (toc - tic), "seconds.")

    return(Fmodels['model'][len(Fmodels['model'])])
```


```python
Forward_best_model = forward_model(X=x_data_S5_2, y= w)
```

    Processed  6 models on 1 predictors in 0.015956878662109375
    Selected predictors: ['RA', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002E949E27488>
    AIC: 9097.598863115662
    Processed  5 models on 2 predictors in 0.00797724723815918
    Selected predictors: ['RA', 'RS', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002E949C5BC88>
    AIC: 6902.513504325496
    Processed  4 models on 3 predictors in 0.00797891616821289
    Selected predictors: ['RA', 'RS', 'G', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002E949E2EA88>
    AIC: 6899.191986626114
    Processed  3 models on 4 predictors in 0.005984306335449219
    Selected predictors: ['RA', 'RS', 'G', 'OBP', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002E949E216C8>
    AIC: 6897.052145582751
    Processed  2 models on 5 predictors in 0.00498652458190918
    Selected predictors: ['RA', 'RS', 'G', 'OBP', 'SLG', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002E949E24608>
    AIC: 6894.540831416643
    Processed  1 models on 6 predictors in 0.0029921531677246094
    Selected predictors: ['RA', 'RS', 'G', 'OBP', 'SLG', 'BA', 'const']  AIC: <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x000002E949E24088>
    AIC: 6894.258422266606
    Total elapsed time: 0.07280516624450684 seconds.
    

    C:\Users\drogpard\Anaconda3\lib\site-packages\ipykernel_launcher.py:15: FutureWarning: 
    The current behaviour of 'Series.argmin' is deprecated, use 'idxmin'
    instead.
    The behavior of 'argmin' will be corrected to return the positional
    minimum in the future. For now, use 'series.values.argmin' or
    'np.argmin(np.array(values))' to get the position of the minimum
    row.
      from ipykernel import kernelapp as app
    


```python
Forward_best_model.aic
```




    6894.258422266606




```python
Forward_best_model.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>W</td>        <th>  R-squared:         </th> <td>   0.881</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.881</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1514.</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 28 Sep 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>16:50:09</td>     <th>  Log-Likelihood:    </th> <td> -3440.1</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1232</td>      <th>  AIC:               </th> <td>   6894.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  1225</td>      <th>  BIC:               </th> <td>   6930.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>RA</th>    <td>   -0.1050</td> <td>    0.001</td> <td>  -77.700</td> <td> 0.000</td> <td>   -0.108</td> <td>   -0.102</td>
</tr>
<tr>
  <th>RS</th>    <td>    0.0912</td> <td>    0.005</td> <td>   19.957</td> <td> 0.000</td> <td>    0.082</td> <td>    0.100</td>
</tr>
<tr>
  <th>G</th>     <td>    0.5040</td> <td>    0.184</td> <td>    2.742</td> <td> 0.006</td> <td>    0.143</td> <td>    0.865</td>
</tr>
<tr>
  <th>OBP</th>   <td>   58.5427</td> <td>   20.365</td> <td>    2.875</td> <td> 0.004</td> <td>   18.588</td> <td>   98.497</td>
</tr>
<tr>
  <th>SLG</th>   <td>   22.9386</td> <td>    9.409</td> <td>    2.438</td> <td> 0.015</td> <td>    4.480</td> <td>   41.397</td>
</tr>
<tr>
  <th>BA</th>    <td>  -27.0409</td> <td>   17.942</td> <td>   -1.507</td> <td> 0.132</td> <td>  -62.241</td> <td>    8.159</td>
</tr>
<tr>
  <th>const</th> <td>  -12.0766</td> <td>   30.741</td> <td>   -0.393</td> <td> 0.695</td> <td>  -72.388</td> <td>   48.235</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.385</td> <th>  Durbin-Watson:     </th> <td>   2.055</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.825</td> <th>  Jarque-Bera (JB):  </th> <td>   0.297</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.025</td> <th>  Prob(JB):          </th> <td>   0.862</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.057</td> <th>  Cond. No.          </th> <td>2.87e+05</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.87e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems.




```python
# ì°¸ê³ : ê²½ê¸°ìˆ˜ G íˆìŠ¤í† ê·¸ë¨
moneyball["G"].value_counts().plot.barh()
# ëŒ€ë¶€ë¶„ 162 ê·¼ì²˜ì— ìˆìŒ
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2e949da3088>




    
![png](/assets/images/output_64_1.png)
    



```python
# ì°¸ê³ ë¡œ ì„ í˜•íšŒê·€ë¶„ì„ì˜ ë‹¤ì–‘í•œ í†µê³„ì ì¸ ê²°ê³¼ ìˆ˜ì¹˜ë¥¼ ì›ì¹˜ ì•ŠëŠ” ê²½ìš°
# sklearnì„ ì´ìš©í•´ì„œ ì†ì‰½ê²Œ íšŒê·€ë¶„ì„ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

from sklearn.linear_model import LinearRegression

# Fit í•  ë•Œ ìƒìˆ˜í•­ì„ ë”°ë¡œ ì¶”ê°€í•  í•„ìš” ì—†ìŒ
reg = LinearRegression().fit(moneyball[["RS", "RA"]], moneyball["W"])
print("r squared:", reg.score(moneyball[["RS", "RA"]], moneyball["W"]))
print("coefficients: ", reg.coef_)
print("intercept: ", reg.intercept_)
```

    r squared: 0.8795651418768365
    coefficients:  [ 0.10449347 -0.10460008]
    intercept:  80.98045555972712
    

## STEP 6. ë¡œì§€ìŠ¤í‹±íšŒê·€ ëª¨ë¸ ìƒì„±
- RSì™€ RA ë‘ ë…ë¦½ë³€ìˆ˜ë¡œ í”Œë ˆì´ì˜¤í”„(Playoffs) ì§„ì¶œ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ ëª¨ë¸ì„ ìƒì„±í•´ë³´ì‹œì˜¤.
- Confusion matrix, AUC, ROC ê³¡ì„ ì„ ì´ìš©í•´ì„œ ê²°ê³¼ë¥¼ í•´ì„í•´ ë³´ì‹œì˜¤.
- ì°¸ê³ 
    - [Scikit learn confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)
    - [Model Evaluation Techniques for Classification models](https://towardsdatascience.com/model-evaluation-techniques-for-classification-models-eac30092c38b)
    - [Evaluating a Classification Model]()

### [í’€ì´] 
- ì •í™•ë„ ACC ëŠ” 0.885551948051948
- ROC ì»¤ë¸ŒëŠ” ì™¼ìª½ ëª¨ì„œë¦¬ ìª½ìœ¼ë¡œ ê°€ê¹Œìš¸ ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ìŒ
- AUC ëŠ” 0ê³¼ 1ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ëŠ” ê·¸ë˜í”„ì˜ ë©´ì  ê°’ì´ë‹¤.


```python
# í”Œë ˆì´ì˜¤í”„ ì§„ì¶œ ì—¬ë¶€: 1 or 0
y = moneyball['Playoffs']
y
```




    0       0
    1       1
    2       1
    3       0
    4       0
           ..
    1227    0
    1228    0
    1229    1
    1230    0
    1231    0
    Name: Playoffs, Length: 1232, dtype: int64




```python
## ë¡œì§€ìŠ¤í‹± ëª¨í˜• ì í•© 
model = sm.Logit(y, x_data1)
fit_logit = model.fit(method="newton")
```

    Optimization terminated successfully.
             Current function value: 0.256725
             Iterations 8
    


```python
fit_logit.summary()
```




<table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>Playoffs</td>     <th>  No. Observations:  </th>   <td>  1232</td>  
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  1229</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Mon, 28 Sep 2020</td> <th>  Pseudo R-squ.:     </th>   <td>0.4842</td>  
</tr>
<tr>
  <th>Time:</th>                <td>16:50:11</td>     <th>  Log-Likelihood:    </th>  <td> -316.28</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -613.15</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.178e-129</td>
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   -5.0982</td> <td>    0.980</td> <td>   -5.200</td> <td> 0.000</td> <td>   -7.020</td> <td>   -3.177</td>
</tr>
<tr>
  <th>RS</th>    <td>    0.0327</td> <td>    0.002</td> <td>   14.422</td> <td> 0.000</td> <td>    0.028</td> <td>    0.037</td>
</tr>
<tr>
  <th>RA</th>    <td>   -0.0300</td> <td>    0.002</td> <td>  -13.465</td> <td> 0.000</td> <td>   -0.034</td> <td>   -0.026</td>
</tr>
</table>




```python
fit_logit.params
```




    const   -5.098231
    RS       0.032685
    RA      -0.029999
    dtype: float64




```python
np.exp(fit_logit.params)
```




    const    0.006108
    RS       1.033225
    RA       0.970446
    dtype: float64




```python
## y_hat ì˜ˆì¸¡
pred_y = fit_logit.predict(x_data1)
pred_y
```




    0       0.148442
    1       0.445659
    2       0.048525
    3       0.005032
    4       0.000397
              ...   
    1227    0.007965
    1228    0.309577
    1229    0.947836
    1230    0.569673
    1231    0.000912
    Length: 1232, dtype: float64




```python
def cut_off(y,threshold):
    Y = y.copy() # copyí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ì˜ yê°’ì´ ë³€í™”ì§€ ì•Šê²Œ í•¨
    Y[Y>threshold]=1
    Y[Y<=threshold]=0
    return(Y.astype(int))

pred_Y = cut_off(pred_y,0.5)
pred_Y
```




    0       0
    1       0
    2       0
    3       0
    4       0
           ..
    1227    0
    1228    0
    1229    1
    1230    1
    1231    0
    Length: 1232, dtype: int32




```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve
import statsmodels.api as sm

# confusion matrix
cfmat = confusion_matrix(y, pred_Y)
print(cfmat)
```

    [[940  48]
     [ 93 151]]
    


```python
(cfmat[0,0]+cfmat[1,1])/np.sum(cfmat) ## accuracy
```




    0.885551948051948




```python
def acc(cfmat) :
    acc=(cfmat[0,0]+cfmat[1,1])/np.sum(cfmat) ## accuracy
    return(acc)
```


```python
acc(cfmat)
```




    0.885551948051948




```python
threshold = np.arange(0,1,0.1)
table = pd.DataFrame(columns=['ACC'])
for i in threshold:
    pred_Y = cut_off(pred_y,i)
    cfmat = confusion_matrix(y, pred_Y)
    table.loc[i] = acc(cfmat)
table.index.name='threshold'
table.columns.name='performance'
table
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>performance</th>
      <th>ACC</th>
    </tr>
    <tr>
      <th>threshold</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.0</td>
      <td>0.198052</td>
    </tr>
    <tr>
      <td>0.1</td>
      <td>0.780844</td>
    </tr>
    <tr>
      <td>0.2</td>
      <td>0.839286</td>
    </tr>
    <tr>
      <td>0.3</td>
      <td>0.870942</td>
    </tr>
    <tr>
      <td>0.4</td>
      <td>0.885552</td>
    </tr>
    <tr>
      <td>0.5</td>
      <td>0.885552</td>
    </tr>
    <tr>
      <td>0.6</td>
      <td>0.873377</td>
    </tr>
    <tr>
      <td>0.7</td>
      <td>0.866071</td>
    </tr>
    <tr>
      <td>0.8</td>
      <td>0.855519</td>
    </tr>
    <tr>
      <td>0.9</td>
      <td>0.835227</td>
    </tr>
  </tbody>
</table>
</div>




```python
# sklearn ROC íŒ¨í‚¤ì§€ ì œê³µ
fpr, tpr, thresholds = metrics.roc_curve(y, pred_y, pos_label=1)

# Print ROC curve
plt.plot(fpr,tpr)

# Print AUC
auc = np.trapz(tpr,fpr)
print('AUC:', auc)

```

    AUC: 0.9330428253799695
    


    
![png](/assets/images/output_80_1.png)
    



```python
# ì°¸ê³ ë¡œ ì„ í˜•íšŒê·€ë¶„ì„ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ sklearnì„ ì´ìš©í•´ì„œ ì†ì‰½ê²Œ íšŒê·€ë¶„ì„ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.
# ìœ„ ê²°ê³¼ì™€ ë¹„êµí•´ë³´ì‹œì˜¤.

from sklearn.linear_model import LogisticRegression

# Fit í•  ë•Œ ìƒìˆ˜í•­ì„ ë”°ë¡œ ì¶”ê°€í•  í•„ìš” ì—†ìŒ
reg = LogisticRegression().fit(moneyball[["RS", "RA"]], moneyball["Playoffs"])
print("mean accuracy:", reg.score(moneyball[["RS", "RA"]], moneyball["Playoffs"]))
print("coefficients: ", reg.coef_)
print("intercept: ", reg.intercept_)
```

    mean accuracy: 0.8758116883116883
    coefficients:  [[ 0.02833851 -0.03250847]]
    intercept:  [-0.08637473]
    

    C:\Users\drogpard\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
      FutureWarning)
    

### Comments
- ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ì€ ì£¼ì–´ì§„ ë…ë¦½ë³€ìˆ˜ì˜ ê³µê°„ì„ ì„ í˜•ìœ¼ë¡œ ë¶„ë¦¬í•œë‹¤ê³  ìˆ˜í•™ì ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤. ì¡°ê¸ˆ ë” ìì„¸íˆ ì„¤ëª…í•˜ë©´ ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ì˜ ê²°ê³¼ë¡œ ìš°ë¦¬ëŠ” ì•„ë˜ ì‹ì˜ ê³„ìˆ˜ë“¤ $\alpha_1, \alpha_2, \beta$ë¥¼ ì–»ì—ˆë‹¤. í”Œë ˆì´ì˜¤í”„ì˜ ì§„ì¶œ ì—¬ë¶€ë¥¼ 0 ~ 1ì‚¬ì´ì˜ í™•ë¥  ê°’ìœ¼ë¡œ ì¶œë ¥í•˜ëŠ” ì‹ì´ ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ì˜ ê²°ê³¼ì´ë‹¤. ì—¬ê¸°ì„œ ì§„ì¶œ ì—¬ë¶€ì˜ íŒë‹¨ ê¸°ì¤€ì„ $1/2$ë¡œ í•œë‹¤ê³  ê°€ì •í•´ë³´ì(ì¼ë°˜ì ìœ¼ë¡œ $1/2$ í™•ë¥ ë¡œ íŒë‹¨í•˜ì§€ë§Œ ëª¨ë¸ì´ë‚˜ ìƒí™©ì— ë”°ë¼ì„œ ì¡°ì ˆí•  ìˆ˜ë„ ìˆë‹¤).

$$
\text{Playoffs} = \frac{1}{1 + e^{(\alpha_1 RS + \alpha_2 RA + \beta)}}
$$

- ì™¼ìª½ í”Œë ˆì´ì˜¤í”„ ì§„ì¶œ í™•ë¥ ì„ 1/2ë¡œ ë‘ê³  ì‹ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\alpha_1 RS + \alpha_2 RA + \beta = 0
$$

- ê²°ê³¼ì ìœ¼ë¡œ ìœ„ ì‹ì„ ë§Œì¡±í•˜ëŠ” RSì™€ RA ê°’ë“¤ì€ ì•„ë˜ì™€ ê°™ì€ ë¹¨ê°„ìƒ‰ ì„ ê³¼ ê°™ì€ ì§ì„ ì˜ í˜•íƒœë¥¼ ë¤ë‹¤. ì§ì„ (ì„ í˜•)ì„ ê¸°ì¤€ìœ¼ë¡œ í•œìª½ ì˜ì—­ì€ í”Œë ˆì´ì˜¤í”„ ì§„ì¶œ ëª»í•¨, ë°˜ëŒ€ìª½ì€ í”Œë ˆì´ì˜¤í”„ ì§„ì¶œí•¨ìœ¼ë¡œ íŒë‹¨ë˜ëŠ” ê²ƒì´ë‹¤(ë¬¼ë¡  ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ëª¨ë¸ì˜ íŒë‹¨). 

![](https://mjgim-fc.s3.ap-northeast-2.amazonaws.com/logistic.png)


- ì´ì²˜ëŸ¼ ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ì€ ì£¼ì–´ì§„ ë…ë¦½ë³€ìˆ˜ ê³µê°„ì„ ì„ í˜•ìœ¼ë¡œ ë¶„ë¦¬í•´ì„œ ì´ì§„ ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì— XOR ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ì–´ë ¤ìš´ ê²ƒì´ë‹¤(XORì€ ì„ í˜•ìœ¼ë¡œ í•´ê²° ë¶ˆê°€ëŠ¥). ê·¸ë¦¼ ì¶œì²˜: https://web.stanford.edu/~jurafsky/slp3/7.pdf

![](https://mjgim-fc.s3.ap-northeast-2.amazonaws.com/xor.png)


```python
line_x = moneyball["RS"]

line_y = (- fit_logit.params["RS"] * moneyball["RS"]  - fit_logit.params["const"] ) \
          / fit_logit.params["RA"]

moneyball.plot.scatter(x = "RS", y = "RA", alpha = 0.5)
plt.plot(line_x, line_y, "r")
```




    [<matplotlib.lines.Line2D at 0x2e94b929108>]




    
![png](/assets/images/output_83_1.png)
    



```python

```


```python

```

## STEP 7. ë³€ìˆ˜ ì„ íƒë²•ìœ¼ë¡œ ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ ì •í™•ë„ ì˜¬ë¦¬ê¸°
- RS, RA, OBP, SLG, BA, Gì„ ë…ë¦½ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ(ìƒìˆ˜í•­ í¬í•¨) í”Œë ˆì´ì˜¤í”„ ì§„ì¶œ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì‹œì˜¤.
- í›„ì§„ì†Œê±°ë²•ìœ¼ë¡œ ìµœì ì˜ ë…ë¦½ë³€ìˆ˜ë¥¼ ì°¾ì•„ ë³´ì‹œì˜¤(AIC ê°’ì´ í¬ê²Œ).

### [í’€ì´] 
- ì •í™•ë„ ACC ëŠ” 0.8806818181818182


```python
## ë¡œì§€ìŠ¤í‹± ëª¨í˜• ì í•© 
# ìƒìˆ˜í•­ ì¶”ê°€ëœ x_data_S5_2 ë³€ìˆ˜ ì‚¬ìš©
model2 = sm.Logit(y, x_data_S5_2)
fit_logit2 = model2.fit(method="newton")
```

    Optimization terminated successfully.
             Current function value: 0.253002
             Iterations 9
    


```python
fit_logit2.summary()
```




<table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>Playoffs</td>     <th>  No. Observations:  </th>   <td>  1232</td>  
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  1225</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     6</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Mon, 28 Sep 2020</td> <th>  Pseudo R-squ.:     </th>   <td>0.4916</td>  
</tr>
<tr>
  <th>Time:</th>                <td>16:50:12</td>     <th>  Log-Likelihood:    </th>  <td> -311.70</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -613.15</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>5.493e-127</td>
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   -7.0517</td> <td>   29.004</td> <td>   -0.243</td> <td> 0.808</td> <td>  -63.899</td> <td>   49.795</td>
</tr>
<tr>
  <th>RS</th>    <td>    0.0230</td> <td>    0.004</td> <td>    5.311</td> <td> 0.000</td> <td>    0.015</td> <td>    0.031</td>
</tr>
<tr>
  <th>RA</th>    <td>   -0.0311</td> <td>    0.002</td> <td>  -13.331</td> <td> 0.000</td> <td>   -0.036</td> <td>   -0.026</td>
</tr>
<tr>
  <th>OBP</th>   <td>   44.8459</td> <td>   18.453</td> <td>    2.430</td> <td> 0.015</td> <td>    8.679</td> <td>   81.013</td>
</tr>
<tr>
  <th>SLG</th>   <td>   18.9428</td> <td>    8.276</td> <td>    2.289</td> <td> 0.022</td> <td>    2.722</td> <td>   35.163</td>
</tr>
<tr>
  <th>BA</th>    <td>  -22.7653</td> <td>   15.575</td> <td>   -1.462</td> <td> 0.144</td> <td>  -53.292</td> <td>    7.761</td>
</tr>
<tr>
  <th>G</th>     <td>   -0.0411</td> <td>    0.173</td> <td>   -0.238</td> <td> 0.812</td> <td>   -0.380</td> <td>    0.298</td>
</tr>
</table>




```python
fit_logit2.params
```




    const    -7.051707
    RS        0.022982
    RA       -0.031057
    OBP      44.845883
    SLG      18.942805
    BA      -22.765310
    G        -0.041106
    dtype: float64




```python
np.exp(fit_logit2.params)
```




    const    8.659293e-04
    RS       1.023248e+00
    RA       9.694204e-01
    OBP      2.994466e+19
    SLG      1.685605e+08
    BA       1.297631e-10
    G        9.597272e-01
    dtype: float64




```python
## y_hat ì˜ˆì¸¡
pred_y_2 = fit_logit2.predict(x_data_S5_2)
pred_y_2
```




    0       0.185768
    1       0.459821
    2       0.046554
    3       0.003003
    4       0.000351
              ...   
    1227    0.008395
    1228    0.244520
    1229    0.903122
    1230    0.433377
    1231    0.000916
    Length: 1232, dtype: float64




```python
pred_Y_2 = cut_off(pred_y_2,0.5)
pred_Y_2
```




    0       0
    1       0
    2       0
    3       0
    4       0
           ..
    1227    0
    1228    0
    1229    1
    1230    0
    1231    0
    Length: 1232, dtype: int32




```python
# confusion matrix
cfmat2 = confusion_matrix(y, pred_Y_2)
print(cfmat2)
```

    [[938  50]
     [ 97 147]]
    


```python
acc(cfmat2)
```




    0.8806818181818182




```python
threshold2 = np.arange(0,1,0.1)
table2 = pd.DataFrame(columns=['ACC'])
for i in threshold2:
    pred_Y_2 = cut_off(pred_y_2,i)
    cfmat2 = confusion_matrix(y, pred_Y_2)
    table2.loc[i] = acc(cfmat2)
table2.index.name='threshold'
table2.columns.name='performance'
table2
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>performance</th>
      <th>ACC</th>
    </tr>
    <tr>
      <th>threshold</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.0</td>
      <td>0.198052</td>
    </tr>
    <tr>
      <td>0.1</td>
      <td>0.779221</td>
    </tr>
    <tr>
      <td>0.2</td>
      <td>0.840909</td>
    </tr>
    <tr>
      <td>0.3</td>
      <td>0.876623</td>
    </tr>
    <tr>
      <td>0.4</td>
      <td>0.885552</td>
    </tr>
    <tr>
      <td>0.5</td>
      <td>0.880682</td>
    </tr>
    <tr>
      <td>0.6</td>
      <td>0.879870</td>
    </tr>
    <tr>
      <td>0.7</td>
      <td>0.871753</td>
    </tr>
    <tr>
      <td>0.8</td>
      <td>0.854708</td>
    </tr>
    <tr>
      <td>0.9</td>
      <td>0.839286</td>
    </tr>
  </tbody>
</table>
</div>




```python
# sklearn ROC íŒ¨í‚¤ì§€ ì œê³µ
fpr, tpr, thresholds = metrics.roc_curve(y, pred_y_2, pos_label=1)

# Print ROC curve
plt.plot(fpr,tpr)

# Print AUC
auc = np.trapz(tpr,fpr)
print('AUC:', auc)
```

    AUC: 0.9345838587641866
    


    
![png](/assets/images/output_97_1.png)
    



```python
# Fit í•  ë•Œ ìƒìˆ˜í•­ì„ ë”°ë¡œ ì¶”ê°€í•  í•„ìš” ì—†ìŒ
reg2 = LogisticRegression().fit(moneyball[['RS','RA','OBP','SLG','BA','G']], moneyball["Playoffs"])
print("mean accuracy:", reg2.score(moneyball[['RS','RA','OBP','SLG','BA','G']], moneyball["Playoffs"]))
print("coefficients: ", reg2.coef_)
print("intercept: ", reg2.intercept_)
```

    mean accuracy: 0.8863636363636364
    coefficients:  [[ 3.26987675e-02 -2.99923253e-02 -2.59382456e-05  5.58227227e-06
      -3.29660009e-05 -3.15644417e-02]]
    intercept:  [-0.00019282]
    

    C:\Users\drogpard\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
      FutureWarning)
    


```python
def processSubset(X,y, feature_set):
            model = sm.Logit(y,X[list(feature_set)])
            regr = model.fit()
            AIC = regr.aic
            return {"model":regr, "AIC":AIC}

import itertools

'''
í›„ì§„ì†Œê±°ë²•
'''
def backward(X,y,predictors):
    tic = time.time()
    results = []
    
    # ë°ì´í„° ë³€ìˆ˜ë“¤ì´ ë¯¸ë¦¬ì •ì˜ëœ predictors ì¡°í•© í™•ì¸
    for combo in itertools.combinations(predictors, len(predictors) - 1):
        results.append(processSubset(X=X, y= y,feature_set=list(combo)+['const']))
    models = pd.DataFrame(results)
    
    # ê°€ì¥ ë‚®ì€ AICë¥¼ ê°€ì§„ ëª¨ë¸ì„ ì„ íƒ
    best_model = models.loc[models['AIC'].argmin()]
    toc = time.time()
    print("Processed ", models.shape[0], "models on", len(predictors) - 1, "predictors in",
          (toc - tic))
    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )
    return best_model


def backward_model(X, y):
    Bmodels = pd.DataFrame(columns=["AIC", "model"], index = range(1,len(X.columns)))
    tic = time.time()
    predictors = X.columns.difference(['const'])
    Bmodel_before = processSubset(X,y,predictors)['AIC']
    while (len(predictors) > 1):
        Backward_result = backward(X=X, y= y, predictors = predictors)
        if Backward_result['AIC'] > Bmodel_before:
            break
        Bmodels.loc[len(predictors) - 1] = Backward_result
        predictors = Bmodels.loc[len(predictors) - 1]["model"].model.exog_names
        Bmodel_before = Backward_result['AIC']
        print('AIC:',Backward_result['AIC'])
        predictors = [ k for k in predictors if k != 'const']

    toc = time.time()
    print("Total elapsed time:", (toc - tic), "seconds.")
    return (Bmodels['model'].dropna().iloc[0])
```


```python
Backward_best_model = backward_model(X=x_data_S5_2,y=y)
```

    Optimization terminated successfully.
             Current function value: 0.253026
             Iterations 9
    Optimization terminated successfully.
             Current function value: 0.255159
             Iterations 9
    Optimization terminated successfully.
             Current function value: 0.265441
             Iterations 9
    Optimization terminated successfully.
             Current function value: 0.414357
             Iterations 7
    Optimization terminated successfully.
             Current function value: 0.255423
             Iterations 9
    Optimization terminated successfully.
             Current function value: 0.253025
             Iterations 9
    Optimization terminated successfully.
             Current function value: 0.253873
             Iterations 9
    Processed  6 models on 5 predictors in 0.029918670654296875
    Selected predictors: ['BA', 'OBP', 'RA', 'RS', 'SLG', 'const']  AIC: <statsmodels.discrete.discrete_model.BinaryResultsWrapper object at 0x000002E94C9AA2C8>
    AIC: 635.453294139449
    Optimization terminated successfully.
             Current function value: 0.255274
             Iterations 8
    Optimization terminated successfully.
             Current function value: 0.265623
             Iterations 8
    Optimization terminated successfully.
             Current function value: 0.414440
             Iterations 7
    Optimization terminated successfully.
             Current function value: 0.255530
             Iterations 8
    Optimization terminated successfully.
             Current function value: 0.253890
             Iterations 9
    Processed  5 models on 4 predictors in 0.0229339599609375
    Selected predictors: ['OBP', 'RA', 'RS', 'SLG', 'const']  AIC: <statsmodels.discrete.discrete_model.BinaryResultsWrapper object at 0x000002E94C9AE548>
    Total elapsed time: 0.06279873847961426 seconds.
    

    C:\Users\drogpard\Anaconda3\lib\site-packages\ipykernel_launcher.py:22: FutureWarning: 
    The current behaviour of 'Series.argmin' is deprecated, use 'idxmin'
    instead.
    The behavior of 'argmin' will be corrected to return the positional
    minimum in the future. For now, use 'series.values.argmin' or
    'np.argmin(np.array(values))' to get the position of the minimum
    row.
    

## ì°¸ê³ 
- í†µê³„ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— íŠ¹í™”(?)ëœ Rì—ì„œëŠ” íšŒê·€ë¶„ì„ ëª¨ë¸ ìƒì„±, ë³€ìˆ˜ì„ íƒë²•, VIF ë“± í†µê³„ë¶„ì„ì„ ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.
- ìœ„ì—ì„œ ìˆ˜í–‰í–ˆë˜ ë‚´ìš©ì„ Rë¡œ ì‹¤í–‰í•œ ë‚´ìš©ì…ë‹ˆë‹¤.
- [R vs Python íšŒê·€ë¶„ì„ëª¨ë¸](https://s3.ap-northeast-2.amazonaws.com/mjgim.assets/2.+Regression+in+R.html)


```python

```
